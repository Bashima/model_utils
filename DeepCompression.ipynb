{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "from matplotlib import pyplot as plt \n",
    "from skimage import io\n",
    "from keras import backend as K\n",
    "from scipy.cluster.vq import vq, kmeans, whiten, kmeans2\n",
    "from keras.models import load_model\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from json and h5 (has both the model and the weight)\n",
    "def loadModelJsonH5(model_name):\n",
    "#     load the model architecture from the json file\n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    \n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "#     load the model parameters (weights) from the h5 file\n",
    "    loaded_model.load_weights(model_name + \".h5\")\n",
    "    \n",
    "#     print the summary of the model \n",
    "    loaded_model.summary()\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from json (has the model only, no weight)\n",
    "def loadModelJson(model_name):\n",
    "#     load the model architecture from the json file\n",
    "    json_file = open(model_name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    \n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "#     print the summary of the model \n",
    "#     loaded_model.summary()\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from h5 (both model and weight)\n",
    "def loadModelH5(model_name):\n",
    "    filename = model_name + \".h5\";\n",
    "    print(filename)\n",
    "    loaded_model = load_model(filename);\n",
    "#     loaded_model.summary()\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantize the weights\n",
    "#inputs are weights of a layer, number of cluster (how many numbers we can save, 8 usually works well) and 0/1 (1 if we want to vizualize)\n",
    "\n",
    "def quantize_weights(wts, numClusters, viz_codeX):\n",
    "    original_data=np.copy(wts)\n",
    "    nz_idx=np.nonzero(original_data)\n",
    "    nz_data=original_data[nz_idx]\n",
    "    F=nz_data.flatten()\n",
    "    F=F.reshape(-1,1)\n",
    "    InitC=np.linspace(F.min(),F.max(),num=numClusters) #linear initialization is done. According to the paper this gives better result\n",
    "    codebook, codeX=kmeans2(F, InitC.reshape(-1,1), minit='matrix')\n",
    "    \n",
    "    if viz_codeX==1:\n",
    "        print(codebook)\n",
    "        print(len(codeX))\n",
    "        # edges_hist=[x for x in range(numClusters+1)]\n",
    "        # frq, edges = np.histogram(codeX,edges_hist)\n",
    "        # print(frq,edges)\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.bar(edges[:-1], frq, width=np.diff(edges), ec=\"k\", align=\"edge\")\n",
    "        # plt.title(\"cluster value histogram\")\n",
    "        # plt.show()\n",
    "    return codebook, codeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the weight from the codeX and codeVal\n",
    "def create_codeVal(codeX, codebook, wts_shape):\n",
    "    code_val=np.zeros(codeX.shape)\n",
    "    for idx,val in enumerate(codeX):\n",
    "        code_val[idx]=codebook[val]\n",
    "    new_wts=code_val.reshape(wts_shape)\n",
    "    return new_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code for loading a model and quantize_weights\n",
    "# def sample_quantize_weights\n",
    "\n",
    "# model = loadModelJsonH5(\"Models/model3\")\n",
    "def sample_quantize(modelname):\n",
    "    codebooks = []\n",
    "    codeXs = []\n",
    "    cluster_number = 8\n",
    "    model = loadModelH5(modelname)\n",
    "    for layer in model.layers:\n",
    "        print(layer)\n",
    "        existing_weight = layer.get_weights()\n",
    "        existing_weight_np = np.asarray(existing_weight)\n",
    "        print(existing_weight_np.shape)\n",
    "    #     If convolution layer then there is both weight and bias index 0 is weight and index 1 is bias\n",
    "        if existing_weight_np.shape == (2,):\n",
    "            for i in range (0,2):\n",
    "                codebook, codeX = quantize_weights(existing_weight_np[i], cluster_number, 0)\n",
    "                codebooks.append(codebook)\n",
    "                codeXs.append(codeX)\n",
    "        elif existing_weight_np.shape != (0,) :\n",
    "            codebook, codeX = quantize_weights(existing_weight_np, cluster_number, 0)\n",
    "            codebooks.append(codebook)\n",
    "            codeXs.append(codeX)\n",
    "    return codebooks, codeXs\n",
    "#         by saving the codebook, codeX and the shape of the weight we can save space. \n",
    "\n",
    "\n",
    "# this calls the sample function\n",
    "codebook, codeX = sample_quantize(\"tr\")\n",
    "#Now we will save them \n",
    "np.save(\"codebook_file.npy\", codebook)\n",
    "\n",
    "codeX_file = TemporaryFile()\n",
    "np.save(\"codeX_file.npy\", codeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv1D object at 0x7fccbf77d080>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fccbf77d518>\n",
      "<keras.layers.core.Activation object at 0x7fccbf77d630>\n",
      "<keras.layers.pooling.MaxPooling1D object at 0x7fccbf713320>\n",
      "<keras.layers.convolutional.Conv1D object at 0x7fccbf77d5f8>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fccbf609588>\n",
      "<keras.layers.core.Activation object at 0x7fccbf646b70>\n",
      "<keras.layers.pooling.MaxPooling1D object at 0x7fccbf3e9470>\n",
      "<keras.layers.convolutional.Conv1D object at 0x7fccbf389c18>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fccbf2decc0>\n",
      "<keras.layers.core.Activation object at 0x7fccbf609c50>\n",
      "<keras.layers.pooling.MaxPooling1D object at 0x7fccbf2bdeb8>\n",
      "<keras.layers.convolutional.Conv1D object at 0x7fccbf2def98>\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7fccbf1b1f98>\n",
      "<keras.layers.core.Activation object at 0x7fccbf227e48>\n",
      "<keras.layers.pooling.MaxPooling1D object at 0x7fccbf134eb8>\n",
      "<keras.layers.core.Lambda object at 0x7fccbf134208>\n",
      "<keras.layers.core.Dense object at 0x7fccbf0d39e8>\n"
     ]
    }
   ],
   "source": [
    "# Read the quantized value and get the new weights then build the model\n",
    "\n",
    "def sample_create_codeval(model_name, codeX_name, codebooks_name):\n",
    "    model = loadModelJson(model_name)\n",
    "    codeXs = np.load(codeX_name)\n",
    "    codebooks = np.load(codebooks_name)\n",
    "    index = 0\n",
    "    for layer in model.layers:\n",
    "            print(layer)\n",
    "            existing_weight = layer.get_weights()\n",
    "            existing_weight_np = np.asarray(existing_weight)\n",
    "#             print(existing_weight_np.shape)\n",
    "\n",
    "    #         if convolution layer we update both weight and bias\n",
    "            if existing_weight_np.shape == (2,):\n",
    "                new_w = []\n",
    "                for i in range (0,2):\n",
    "                    codebook = codebooks[index]\n",
    "                    codeX = codeXs[index]\n",
    "                    new_weight = create_codeVal(codeX, codebook, existing_weight_np[i].shape)\n",
    "                    index += 1\n",
    "                    new_w.append(new_weight)\n",
    "                new_weights = np.asarray(new_w)\n",
    "    #             all other layers which have parameters\n",
    "            elif existing_weight_np.shape != (0,) :\n",
    "                codebook = codebooks[index]\n",
    "                codeX = codeXs[index]\n",
    "                new_weights = create_codeVal(codeX, codebook, existing_weight_np.shape)\n",
    "                index += 1\n",
    "    #             for any layer with parameter we update the parametes\n",
    "            if existing_weight_np.shape != (0,) :\n",
    "                layer.set_weights(new_weights)\n",
    "\n",
    "#             existing_weight = layer.get_weights()\n",
    "#             existing_weight_np = np.asarray(existing_weight)\n",
    "#             print(existing_weight_np.shape)\n",
    "    return model\n",
    "            \n",
    "# we are calling the sample here which will give us the final model \n",
    "model = sample_create_codeval(\"model4\", \"codeX_file.npy\", \"codebook_file.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
